{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! sh summary.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing required stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from datetime import timedelta\n",
    "\n",
    "import scipy.misc\n",
    "import glob\n",
    "import sys\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper-functions to load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper functions, DO NOT modify this\n",
    "\n",
    "def get_img_array(path):\n",
    "    \"\"\"\n",
    "    Given path of image, returns it's numpy array\n",
    "    \"\"\"\n",
    "    return scipy.misc.imread(path)\n",
    "\n",
    "def get_files(folder):\n",
    "    \"\"\"\n",
    "    Given path to folder, returns list of files in it\n",
    "    \"\"\"\n",
    "    filenames = [file for file in glob.glob(folder+'*/*')]\n",
    "    filenames.sort()\n",
    "    return filenames\n",
    "\n",
    "def get_label(filepath, label2id):\n",
    "    \"\"\"\n",
    "    Files are assumed to be labeled as: /path/to/file/999_frog.png\n",
    "    Returns label for a filepath\n",
    "    \"\"\"\n",
    "    tokens = filepath.split('/')\n",
    "    label = tokens[-1].split('_')[1][:-4]\n",
    "    if label in label2id:\n",
    "        return label2id[label]\n",
    "    else:\n",
    "        sys.exit(\"Invalid label: \" + label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Functions to load data, DO NOT change these\n",
    "\n",
    "def get_labels(folder, label2id):\n",
    "    \"\"\"\n",
    "    Returns vector of labels extracted from filenames of all files in folder\n",
    "    :param folder: path to data folder\n",
    "    :param label2id: mapping of text labels to numeric ids. (Eg: automobile -> 0)\n",
    "    \"\"\"\n",
    "    files = get_files(folder)\n",
    "    y = []\n",
    "    for f in files:\n",
    "        y.append(get_label(f,label2id))\n",
    "    return np.array(y)\n",
    "\n",
    "def one_hot(y, num_classes=10):\n",
    "    \"\"\"\n",
    "    Converts each label index in y to vector with one_hot encoding\n",
    "    \"\"\"\n",
    "    y_one_hot = np.zeros((num_classes, y.shape[0]))\n",
    "    y_one_hot[y, range(y.shape[0])] = 1\n",
    "    return y_one_hot\n",
    "\n",
    "def get_label_mapping(label_file):\n",
    "    \"\"\"\n",
    "    Returns mappings of label to index and index to label\n",
    "    The input file has list of labels, each on a separate line.\n",
    "    \"\"\"\n",
    "    with open(label_file, 'r') as f:\n",
    "        id2label = f.readlines()\n",
    "        id2label = [l.strip() for l in id2label]\n",
    "    label2id = {}\n",
    "    count = 0\n",
    "    for label in id2label:\n",
    "        label2id[label] = count\n",
    "        count += 1\n",
    "    return id2label, label2id\n",
    "\n",
    "def get_images(folder):\n",
    "    \"\"\"\n",
    "    returns numpy array of all samples in folder\n",
    "    each column is a sample resized to 30x30 and flattened\n",
    "    \"\"\"\n",
    "    files = get_files(folder)\n",
    "    images = []\n",
    "    count = 0\n",
    "    \n",
    "    for f in files:\n",
    "        count += 1\n",
    "        if count % 10000 == 0:\n",
    "            print(\"Loaded {}/{}\".format(count,len(files)))\n",
    "        img_arr = get_img_array(f)\n",
    "        img_arr = img_arr.flatten() / 255.0\n",
    "        images.append(img_arr)\n",
    "    X = np.column_stack(images)\n",
    "\n",
    "    return X\n",
    "\n",
    "def get_train_data(data_root_path):\n",
    "    \"\"\"\n",
    "    Return X and y\n",
    "    \"\"\"\n",
    "    train_data_path = data_root_path + 'train'\n",
    "    id2label, label2id = get_label_mapping(data_root_path+'labels.txt')\n",
    "    print(label2id)\n",
    "    X = get_images(train_data_path)\n",
    "    y = get_labels(train_data_path, label2id)\n",
    "    return X, y\n",
    "\n",
    "def save_predictions(filename, y):\n",
    "    \"\"\"\n",
    "    Dumps y into .npy file\n",
    "    \"\"\"\n",
    "    np.save(filename, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'airplane': 0, 'automobile': 1, 'bird': 2, 'cat': 3, 'deer': 4, 'dog': 5, 'frog': 6, 'horse': 7, 'ship': 8, 'truck': 9}\n",
      "Loaded 10000/50000\n",
      "Loaded 20000/50000\n",
      "Loaded 30000/50000\n",
      "Loaded 40000/50000\n",
      "Loaded 50000/50000\n",
      "Loaded 10000/10000\n",
      "Data loading done\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "data_root_path = 'cifar10-hw2/'\n",
    "X_train, Y_train = get_train_data(data_root_path) # this may take a few minutes\n",
    "X_test = get_images(data_root_path + 'test')\n",
    "print('Data loading done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = X_train.T\n",
    "X_test = X_test.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convolutional Layer 1.\n",
    "filter_size1 = 3 \n",
    "num_filters1 = 64\n",
    "\n",
    "# Convolutional Layer 2.\n",
    "filter_size2 = 3\n",
    "num_filters2 = 64\n",
    "\n",
    "# Convolutional Layer 2.\n",
    "filter_size3 = 3\n",
    "num_filters3 = 128\n",
    "\n",
    "# Fully-connected layer.\n",
    "fc_1 = 256                 # Number of neurons in fully-connected layer.\n",
    "fc_2 = 128                 # Number of neurons in fc layer\n",
    "\n",
    "# Number of color channels for the images: 1 channel for gray-scale.\n",
    "num_channels = 3\n",
    "\n",
    "# image dimensions (only squares for now)\n",
    "img_size = 32\n",
    "\n",
    "# Size of image when flattened to a single dimension\n",
    "img_size_flat = img_size * img_size * num_channels\n",
    "\n",
    "# Tuple with height and width of images used to reshape arrays.\n",
    "img_shape = (img_size, img_size)\n",
    "\n",
    "# class info\n",
    "classes = ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']\n",
    "num_classes = len(classes)\n",
    "\n",
    "# batch size\n",
    "batch_size = 64\n",
    "\n",
    "# validation split\n",
    "validation_size = .16\n",
    "\n",
    "# learning rate \n",
    "learning_rate = 0.001\n",
    "\n",
    "# beta\n",
    "beta = 0.01\n",
    "\n",
    "import os\n",
    "# log directory\n",
    "log_dir = os.getcwd()+\"/summary_2\"\n",
    "\n",
    "# how long to wait after validation loss stops improving before terminating training\n",
    "early_stopping = None  # use None if you don't want to implement early stoping\n",
    "\n",
    "# cropped image size\n",
    "img_size_cropped = 24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper-function for plotting images\n",
    "Function used to plot 9 images in a 3x3 grid (or fewer, depending on how many images are passed), and writing the true and predicted classes below each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_images(images, cls_true, cls_pred=None):\n",
    "    \n",
    "    if len(images) == 0:\n",
    "        print(\"no images to show\")\n",
    "        return \n",
    "    else:\n",
    "        random_indices = random.sample(range(len(images)), min(len(images), 9))\n",
    "        \n",
    "    print(images.shape)   \n",
    "    images, cls_true  = zip(*[(images[i], cls_true[i]) for i in random_indices])\n",
    "    \n",
    "    # Create figure with 3x3 sub-plots.\n",
    "    fig, axes = plt.subplots(3, 3)\n",
    "    fig.subplots_adjust(hspace=0.3, wspace=0.3)\n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        # Plot image.\n",
    "        ax.imshow(images[i].reshape(img_size, img_size, num_channels))\n",
    "\n",
    "        # Show true and predicted classes.\n",
    "        if cls_pred is None:\n",
    "            xlabel = \"True: {0}\".format(classes[cls_true[i]])\n",
    "        else:\n",
    "            xlabel = \"True: {0}, Pred: {1}\".format(cls_true[i], cls_pred[i])\n",
    "\n",
    "        # Show the classes as the label on the x-axis.\n",
    "        ax.set_xlabel(xlabel)\n",
    "        \n",
    "        # Remove ticks from the plot.\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    \n",
    "    # Ensure the plot is shown correctly with multiple plots\n",
    "    # in a single Notebook cell.\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot random images to see if data is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 3072)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUQAAAD5CAYAAABfyUzZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wHKMJeLJaTT4wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff74bf74860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the images and labels using our helper-function above.\n",
    "plot_images(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mean = np.mean(X_train, axis = 0)\n",
    "stdDev = np.std(X_train, axis = 0)\n",
    "\n",
    "X_train -= mean\n",
    "X_train /= stdDev        \n",
    "\n",
    "mean = np.mean(X_test, axis = 0)\n",
    "stdDev = np.std(X_test, axis = 0)\n",
    "\n",
    "X_test -= mean\n",
    "X_test /=stdDev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights and Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def new_weights(shape, name):\n",
    "    #return tf.Variable(tf.constant(0.05, shape=shape))\n",
    "    weights = tf.get_variable(name='weights',shape=shape)\n",
    "    #tf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, weights)\n",
    "    return weights\n",
    "    \n",
    "\n",
    "\n",
    "def new_biases(length, name):\n",
    "    return tf.Variable(tf.constant(0.05, shape=[length]),name='biases')\n",
    "\n",
    "one = tf.constant([1],dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_norm(x, n_out, phase_train):\n",
    "    \"\"\"\n",
    "    Batch normalization on convolutional maps.\n",
    "    Ref.: http://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow\n",
    "    Args:\n",
    "        x:           Tensor, 4D BHWD input maps\n",
    "        n_out:       integer, depth of input maps\n",
    "        phase_train: boolean tf.Varialbe, true indicates training phase\n",
    "        scope:       string, variable scope\n",
    "    Return:\n",
    "        normed:      batch-normalized maps\n",
    "    \"\"\"\n",
    "    with tf.variable_scope('batch_norm'):\n",
    "        beta = tf.Variable(tf.constant(0.0, shape=[n_out]),\n",
    "                                     name='beta', trainable=True)\n",
    "        gamma = tf.Variable(tf.constant(1.0, shape=[n_out]),\n",
    "                                      name='gamma', trainable=True)\n",
    "        batch_mean, batch_var = tf.nn.moments(x, [0,1,2], name='moments')\n",
    "        ema = tf.train.ExponentialMovingAverage(decay=0.5)\n",
    "\n",
    "        def mean_var_with_update():\n",
    "            ema_apply_op = ema.apply([batch_mean, batch_var])\n",
    "            with tf.control_dependencies([ema_apply_op]):\n",
    "                return tf.identity(batch_mean), tf.identity(batch_var)\n",
    "            \n",
    "        mean, var = tf.cond(phase_train,\n",
    "                            mean_var_with_update,\n",
    "                            lambda: (ema.average(batch_mean), ema.average(batch_var)))\n",
    "        normed = tf.nn.batch_normalization(x, mean, var, beta, gamma, 1e-3)\n",
    "    return normed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper-function for summaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def variable_summaries(var):\n",
    "    \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean', mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.summary.scalar('stddev', stddev)\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))\n",
    "        tf.summary.histogram('histogram', var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def new_conv_layer(input,              # The previous layer.\n",
    "                   num_input_channels, # Num. channels in prev. layer.\n",
    "                   filter_size,        # Width and height of each filter.\n",
    "                   num_filters,        # Number of filters.\n",
    "                   name,\n",
    "                   use_pooling=True, normalize=True, batch_normalization =False, flip = False, drop_out = False):  \n",
    "\n",
    "    # Shape of the filter-weights for the convolution.\n",
    "    shape = [filter_size, filter_size, num_input_channels, num_filters]\n",
    "\n",
    "    # Create new weights/filters\n",
    "    with tf.variable_scope('weights'):\n",
    "        weights = new_weights(name=name,shape=shape)\n",
    "        variable_summaries(weights)\n",
    "    \n",
    "    # Create new biases, one for each filter.\n",
    "    with tf.variable_scope('biases'):\n",
    "        biases = new_biases(name=name,length=num_filters)\n",
    "        variable_summaries(biases)\n",
    "    \n",
    "    # convolutional layer\n",
    "    with tf.variable_scope('convolution_layer'):\n",
    "        layer = tf.nn.conv2d(input=input,\n",
    "                             filter=weights,\n",
    "                             strides=[1, 1, 1, 1],\n",
    "                             padding='SAME')\n",
    "        # Add the biases to the results of the convolution.\n",
    "        # A bias-value is added to each filter-channel.\n",
    "        layer += biases\n",
    "\n",
    "    \n",
    "     # Adding batch_norm\n",
    "    if batch_normalization == True:\n",
    "        layer = batch_norm(layer,num_filters, phase)\n",
    "    \n",
    "    # Activation\n",
    "    with tf.variable_scope('ReLU'):\n",
    "        layer = tf.nn.relu(layer)\n",
    "    \n",
    "    # Max-pooling\n",
    "    with tf.variable_scope('Max-Pooling'):\n",
    "        if use_pooling:\n",
    "            layer = tf.nn.max_pool(value=layer,\n",
    "                                   ksize=[1, 2, 2, 1],\n",
    "                                   strides=[1, 2, 2, 1],\n",
    "                                   padding='SAME')\n",
    "\n",
    "    # dropout:\n",
    "    if (drop_out == True):\n",
    "        layer = tf.nn.droppout(layer,keep_prob)\n",
    "        \n",
    "    tf.summary.histogram('activations', layer)\n",
    "    return layer, weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flatten_layer(layer):\n",
    "    # Get the shape of the input layer.\n",
    "    layer_shape = layer.get_shape()\n",
    "\n",
    "    num_features = layer_shape[1:4].num_elements()\n",
    "    \n",
    "    layer_flat = tf.reshape(layer, [-1, num_features])\n",
    "\n",
    "    return layer_flat, num_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FC Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def new_fc_layer(input,          # The previous layer.\n",
    "                 num_inputs,     # Num. inputs from prev. layer.\n",
    "                 num_outputs,    # Num. outputs.\n",
    "                 name,\n",
    "                 use_relu=True, drop_out=False): # Use Rectified Linear Unit (ReLU)?\n",
    "\n",
    "    # Create new weights and biases.\n",
    "    with tf.variable_scope('weights'):\n",
    "        weights = new_weights(shape=[num_inputs, num_outputs], name=name)\n",
    "        variable_summaries(weights)\n",
    "    \n",
    "    with tf.variable_scope('biases'):\n",
    "        biases = new_biases(length=num_outputs, name=name)\n",
    "        variable_summaries(biases)\n",
    "\n",
    "    # Calculate the layer as the matrix multiplication of\n",
    "    # the input and weights, and then add the bias-values.\n",
    "    with tf.variable_scope('matmul'):\n",
    "        layer = tf.matmul(input, weights) + biases\n",
    "\n",
    "    # Use ReLU?\n",
    "    if use_relu:\n",
    "        with tf.variable_scope('relu'):\n",
    "            layer = tf.nn.relu(layer)\n",
    "    # dropout:\n",
    "    if (drop_out == True):\n",
    "        layer = tf.nn.droppout(layer,keep_prob)\n",
    "\n",
    "    return layer, weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pre_process_image(image, training):\n",
    "    # This function takes a single image as input,\n",
    "    # and a boolean whether to build the training or testing graph.\n",
    "    \n",
    "    if training:\n",
    "        # For training, add the following to the TensorFlow graph.\n",
    "\n",
    "        # Randomly crop the input image.\n",
    "        image = tf.random_crop(image, size=[img_size_cropped, img_size_cropped, num_channels])\n",
    "\n",
    "        # Randomly flip the image horizontally.\n",
    "        image = tf.image.random_flip_left_right(image)\n",
    "        \n",
    "        # Randomly adjust hue, contrast and saturation.\n",
    "        image = tf.image.random_hue(image, max_delta=0.05)\n",
    "        image = tf.image.random_contrast(image, lower=0.3, upper=1.0)\n",
    "        image = tf.image.random_brightness(image, max_delta=0.2)\n",
    "        image = tf.image.random_saturation(image, lower=0.0, upper=2.0)\n",
    "\n",
    "        # Some of these functions may overflow and result in pixel\n",
    "        # values beyond the [0, 1] range. It is unclear from the\n",
    "        # documentation of TensorFlow 0.10.0rc0 whether this is\n",
    "        # intended. A simple solution is to limit the range.\n",
    "\n",
    "        # Limit the image pixels between [0, 1] in case of overflow.\n",
    "        image = tf.minimum(image, 1.0)\n",
    "        image = tf.maximum(image, 0.0)\n",
    "    else:\n",
    "        # For training, add the following to the TensorFlow graph.\n",
    "\n",
    "        # Crop the input image around the centre so it is the same\n",
    "        # size as images that are randomly cropped during training.\n",
    "        image = tf.image.resize_image_with_crop_or_pad(image,\n",
    "                                                       target_height=img_size_cropped,\n",
    "                                                       target_width=img_size_cropped)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pre_process(images, training):\n",
    "    # Use TensorFlow to loop over all the input images and call\n",
    "    # the function above which takes a single image as input.\n",
    "    images = tf.map_fn(lambda image: pre_process_image(image, training), images)\n",
    "\n",
    "    return images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Placeholder variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, img_size_flat], name='x')\n",
    "\n",
    "x_image = tf.reshape(x, [-1, img_size, img_size, num_channels])\n",
    "\n",
    "phase = tf.placeholder(tf.bool, name='phase')\n",
    "\n",
    "train_flag = True\n",
    "\n",
    "def set_true(): \n",
    "    global train_flag\n",
    "    train_flag = True\n",
    "    return True\n",
    "\n",
    "def set_false(): \n",
    "    global train_flag\n",
    "    train_flag = False\n",
    "    return False\n",
    "    \n",
    "x_image = tf.cond(phase, lambda: pre_process(x_image, training=True), lambda: pre_process(x_image, training=False))\n",
    "    \n",
    "y_true = tf.placeholder(tf.float32, shape=[None, num_classes], name='y_true')\n",
    "\n",
    "y_true_cls = tf.argmax(y_true, axis=1)\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32, name='keep_prob')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('Layer-1'):\n",
    "    layer_conv1, weights_conv1 = \\\n",
    "        new_conv_layer(input=x_image,\n",
    "                       num_input_channels=num_channels,\n",
    "                       filter_size=filter_size1,\n",
    "                       num_filters=num_filters1,\n",
    "                       name='Conv-Layer1',\n",
    "                       use_pooling=True, batch_normalization=True)\n",
    "        \n",
    "with tf.variable_scope('Layer-2'):\n",
    "    layer_conv2, weights_conv2 = \\\n",
    "        new_conv_layer(input=layer_conv1,\n",
    "                       num_input_channels=num_filters1,\n",
    "                       filter_size=filter_size2,\n",
    "                       num_filters=num_filters2,\n",
    "                       name='Conv-Layer2',\n",
    "                       use_pooling=True, drop_out=False)\n",
    "         \n",
    "with tf.variable_scope('Layer-3'):\n",
    "    layer_conv3, weights_conv3 = \\\n",
    "        new_conv_layer(input=layer_conv2,\n",
    "                       num_input_channels=num_filters2,\n",
    "                       filter_size=filter_size3,\n",
    "                       num_filters=num_filters3,\n",
    "                       name='Conv-Layer3',\n",
    "                       use_pooling=True, drop_out=False) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Flatten/Reshape:0\", shape=(?, 18432), dtype=float32) 18432\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope('Flatten'):\n",
    "    layer_flat, num_features = flatten_layer(layer_conv3)\n",
    "\n",
    "print(layer_flat,num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FC Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('Fully-Connected-1'):\n",
    "    layer_fc1, weights_fc1 = new_fc_layer(input=layer_flat,\n",
    "                             num_inputs=num_features,\n",
    "                             num_outputs=fc_1,\n",
    "                             use_relu=True, drop_out=False, name='FC1')\n",
    "    \n",
    "with tf.variable_scope('Fully-Connected-2'):\n",
    "    layer_fc2, weights_fc2 = new_fc_layer(input=layer_fc1,\n",
    "                             num_inputs=fc_1,\n",
    "                             num_outputs=fc_2,\n",
    "                             use_relu=True, drop_out=False, name='FC2')\n",
    "    \n",
    "with tf.variable_scope('Fully-connected-3'):\n",
    "    layer_fc3, weights_fc3 = new_fc_layer(input=layer_fc2,\n",
    "                             num_inputs=fc_2,\n",
    "                             num_outputs=num_classes,\n",
    "                             use_relu=True, name='FC3')\n",
    "    \n",
    "#with tf.variable_scope('dropout'):\n",
    "#    layer = tf.nn.dropout(layer_fc2,keep_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax and argmax functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('Softmax'):\n",
    "    y_pred = tf.nn.softmax(layer_fc3)\n",
    "    y_pred_cls = tf.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost-Function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Total-Loss:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.variable_scope('cross_entropy_loss'):\n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=layer_fc3,\n",
    "                                                        labels=y_true)\n",
    "\n",
    "    loss = tf.reduce_mean(cross_entropy)\n",
    "    tf.summary.scalar('cross_entropy', loss)\n",
    "        \n",
    "with tf.variable_scope('Regularization'):\n",
    "    regularizers = tf.nn.l2_loss(weights_conv1) + tf.nn.l2_loss(weights_conv2) + tf.nn.l2_loss(weights_fc1) + \\\n",
    "                   tf.nn.l2_loss(weights_fc2) + tf.nn.l2_loss(weights_fc3)\n",
    "    loss = tf.reduce_mean(loss + beta * regularizers)\n",
    "\n",
    "    \n",
    "cost = loss\n",
    "tf.summary.scalar('Total-Loss', cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using SGD Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('SGD'):\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    start_learning_rate = 0.01\n",
    "    learning_rate = tf.train.exponential_decay(start_learning_rate, global_step, 20000, 0.96, staircase=True)\n",
    "    sgd = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    optimizer = sgd.minimize(loss, global_step=global_step)\n",
    "        \n",
    "    tf.summary.histogram(\"conv2d_1_weights_grad\",\n",
    "                             sgd.compute_gradients(loss=loss,\n",
    "                                                         var_list=\n",
    "                                                         [v for v in tf.trainable_variables() if v.name == 'Layer-1/weights/weights:0'])[0]\n",
    "                            )\n",
    "    tf.summary.histogram(\"conv2d_2_weights_grad\",\n",
    "                             sgd.compute_gradients(loss=loss,\n",
    "                                                         var_list=\n",
    "                                                         [v for v in tf.trainable_variables() if v.name == 'Layer-2/weights/weights:0'])[0]\n",
    "                            )\n",
    "    tf.summary.histogram(\"conv2d_3_weights_grad\",\n",
    "                             sgd.compute_gradients(loss=loss,\n",
    "                                                         var_list=\n",
    "                                                         [v for v in tf.trainable_variables() if v.name == 'Layer-3/weights/weights:0'])[0]\n",
    "                            )\n",
    "    tf.summary.histogram(\"dense_weights_grad\",\n",
    "                            sgd.compute_gradients(loss=loss,\n",
    "                                                        var_list=\n",
    "                                                        [v for v in tf.trainable_variables() if v.name == 'Fully-connected-3/weights/weights:0'])[0]\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('Metrics'):\n",
    "    correct_prediction = tf.equal(y_pred_cls, y_true_cls)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    tf.summary.scalar('accuracy', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "session = tf.Session()\n",
    "session.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merged = tf.summary.merge_all()\n",
    "train_writer = tf.summary.FileWriter(log_dir + '/train', session.graph)\n",
    "test_writer = tf.summary.FileWriter(log_dir + '/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 3072)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot(y, num_classes=10):\n",
    "    \"\"\"\n",
    "    Converts each label index in y to vector with one_hot encoding\n",
    "    \"\"\"\n",
    "    y_one_hot = np.zeros((num_classes, y.shape[0]))\n",
    "    y_one_hot[y, range(y.shape[0])] = 1\n",
    "    return y_one_hot\n",
    "\n",
    "Y_hot = one_hot(Y_train)\n",
    "Y_hot = Y_hot.T\n",
    "# split test and train:\n",
    "x_dev_batch = X_train[0:5000,:]\n",
    "y_dev_batch = Y_hot[0:5000,:]\n",
    "X_train = X_train[5000:,:]\n",
    "Y_hot = Y_hot[5000:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "save_dir = 'checkpoints/'\n",
    "\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "save_path = os.path.join(save_dir, 'cifar10_cnn')\n",
    "\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_batch_size = batch_size\n",
    "\n",
    "def print_status(epoch, feed_dict_train, feed_dict_validate, step, max_acc):\n",
    "    # Calculate the accuracy on the training-set.\n",
    "    train_loss, summary, acc = session.run([cost, merged,accuracy], feed_dict=feed_dict_train)\n",
    "    train_writer.add_summary(summary, step)\n",
    "    val_loss, summary, val_acc = session.run([cost, merged,accuracy], feed_dict=feed_dict_validate)\n",
    "    test_writer.add_summary(summary, step)\n",
    "    msg = \"Epoch {0} --- Training Accuracy: {1:>6.1%}, Validation Accuracy: {2:>6.1%}, Train Loss: {3:.3f} Validation Loss: {4:.3f}\"\n",
    "    print(msg.format(epoch + 1, acc, val_acc, train_loss, val_loss))\n",
    "    \n",
    "    if max_acc < val_acc:\n",
    "        max_acc = val_acc\n",
    "        saver.save(session,\n",
    "                   save_path=save_path,\n",
    "                   global_step=step)\n",
    "\n",
    "        print(\"Saved checkpoint.\")\n",
    "    return max_acc\n",
    "    \n",
    "# Counter for total number of iterations performed so far.\n",
    "total_iterations = 0\n",
    "\n",
    "def get_batch(X, Y, batch_size):\n",
    "    \"\"\"\n",
    "    Return minibatch of samples and labels\n",
    "        \n",
    "    :param X, y: samples and corresponding labels\n",
    "    :parma batch_size: minibatch size\n",
    "    :returns: (tuple) X_batch, y_batch\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a random index.\n",
    "    idx = np.random.choice(len(X_train),\n",
    "                           size=batch_size,\n",
    "                           replace=False)\n",
    "\n",
    "\n",
    "    X_batch = X[idx,:]\n",
    "    Y_batch = Y[idx,:]\n",
    "    return X_batch, Y_batch\n",
    "\n",
    "max_acc = 0\n",
    "\n",
    "def optimize(num_iterations):\n",
    "    # Ensure we update the global variable rather than a local copy.\n",
    "    global total_iterations\n",
    "    \n",
    "    best_val_loss = float(\"inf\")\n",
    "    patience = 0\n",
    "\n",
    "    global max_acc\n",
    "    for i in range(total_iterations,\n",
    "                   total_iterations + num_iterations):\n",
    "\n",
    "\n",
    "        x_batch, y_true_batch = get_batch(X_train,Y_hot, train_batch_size)\n",
    "\n",
    "        feed_dict_train = {x: x_batch,\n",
    "                           y_true: y_true_batch, phase: True, keep_prob:0.5}\n",
    "        \n",
    "        feed_dict_validate = {x: x_dev_batch,\n",
    "                              y_true: y_dev_batch, phase: False, keep_prob:1.0}\n",
    "\n",
    "\n",
    "        acc = session.run(optimizer, feed_dict=feed_dict_train)\n",
    "        \n",
    "        # Print status at end of each epoch (defined as full pass through training dataset).\n",
    "        if i % int(X_train.shape[0]/batch_size) == 0 == 0: \n",
    "            epoch = int(i / int(X_train.shape[0]/batch_size))\n",
    "            print('Iteration:',i)\n",
    "            max_acc = print_status(epoch, feed_dict_train, feed_dict_validate, i, max_acc)\n",
    "            \n",
    "    # Update the total number of iterations performed.\n",
    "    total_iterations += num_iterations\n",
    "\n",
    "    # close the writers\n",
    "    train_writer.close()\n",
    "    test_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "Epoch 1 --- Training Accuracy:  25.0%, Validation Accuracy:  12.5%, Train Loss: 6.061 Validation Loss: 6.122\n",
      "Saved checkpoint.\n",
      "Iteration: 703\n",
      "Epoch 2 --- Training Accuracy:  34.4%, Validation Accuracy:  42.5%, Train Loss: 5.250 Validation Loss: 4.944\n",
      "Saved checkpoint.\n",
      "Iteration: 1406\n",
      "Epoch 3 --- Training Accuracy:  50.0%, Validation Accuracy:  44.4%, Train Loss: 4.288 Validation Loss: 4.545\n",
      "Saved checkpoint.\n",
      "Iteration: 2109\n",
      "Epoch 4 --- Training Accuracy:  45.3%, Validation Accuracy:  48.3%, Train Loss: 4.147 Validation Loss: 4.053\n",
      "Saved checkpoint.\n",
      "Iteration: 2812\n",
      "Epoch 5 --- Training Accuracy:  40.6%, Validation Accuracy:  50.0%, Train Loss: 3.748 Validation Loss: 3.661\n",
      "Saved checkpoint.\n",
      "Iteration: 3515\n",
      "Epoch 6 --- Training Accuracy:  57.8%, Validation Accuracy:  54.6%, Train Loss: 3.368 Validation Loss: 3.255\n",
      "Saved checkpoint.\n",
      "Iteration: 4218\n",
      "Epoch 7 --- Training Accuracy:  50.0%, Validation Accuracy:  56.9%, Train Loss: 3.143 Validation Loss: 2.964\n",
      "Saved checkpoint.\n",
      "Iteration: 4921\n",
      "Epoch 8 --- Training Accuracy:  56.2%, Validation Accuracy:  56.1%, Train Loss: 2.954 Validation Loss: 2.832\n",
      "Iteration: 5624\n",
      "Epoch 9 --- Training Accuracy:  56.2%, Validation Accuracy:  59.2%, Train Loss: 2.725 Validation Loss: 2.542\n",
      "Saved checkpoint.\n",
      "Iteration: 6327\n",
      "Epoch 10 --- Training Accuracy:  60.9%, Validation Accuracy:  56.8%, Train Loss: 2.390 Validation Loss: 2.450\n",
      "Iteration: 7030\n",
      "Epoch 11 --- Training Accuracy:  70.3%, Validation Accuracy:  61.8%, Train Loss: 2.073 Validation Loss: 2.178\n",
      "Saved checkpoint.\n",
      "Iteration: 7733\n",
      "Epoch 12 --- Training Accuracy:  57.8%, Validation Accuracy:  59.0%, Train Loss: 2.102 Validation Loss: 2.220\n",
      "Iteration: 8436\n",
      "Epoch 13 --- Training Accuracy:  62.5%, Validation Accuracy:  64.2%, Train Loss: 2.032 Validation Loss: 1.915\n",
      "Saved checkpoint.\n",
      "Iteration: 9139\n",
      "Epoch 14 --- Training Accuracy:  59.4%, Validation Accuracy:  64.1%, Train Loss: 2.019 Validation Loss: 1.866\n",
      "Iteration: 9842\n",
      "Epoch 15 --- Training Accuracy:  65.6%, Validation Accuracy:  66.0%, Train Loss: 1.864 Validation Loss: 1.715\n",
      "Saved checkpoint.\n",
      "Iteration: 10545\n",
      "Epoch 16 --- Training Accuracy:  73.4%, Validation Accuracy:  66.2%, Train Loss: 1.509 Validation Loss: 1.653\n",
      "Saved checkpoint.\n",
      "Iteration: 11248\n",
      "Epoch 17 --- Training Accuracy:  73.4%, Validation Accuracy:  66.2%, Train Loss: 1.512 Validation Loss: 1.611\n",
      "Iteration: 11951\n",
      "Epoch 18 --- Training Accuracy:  62.5%, Validation Accuracy:  66.2%, Train Loss: 1.673 Validation Loss: 1.563\n",
      "Iteration: 12654\n",
      "Epoch 19 --- Training Accuracy:  65.6%, Validation Accuracy:  63.4%, Train Loss: 1.448 Validation Loss: 1.578\n",
      "Iteration: 13357\n",
      "Epoch 20 --- Training Accuracy:  68.8%, Validation Accuracy:  67.2%, Train Loss: 1.384 Validation Loss: 1.461\n",
      "Saved checkpoint.\n",
      "Iteration: 14060\n",
      "Epoch 21 --- Training Accuracy:  73.4%, Validation Accuracy:  66.7%, Train Loss: 1.254 Validation Loss: 1.417\n",
      "Iteration: 14763\n",
      "Epoch 22 --- Training Accuracy:  64.1%, Validation Accuracy:  67.2%, Train Loss: 1.565 Validation Loss: 1.398\n",
      "Iteration: 15466\n",
      "Epoch 23 --- Training Accuracy:  73.4%, Validation Accuracy:  69.5%, Train Loss: 1.258 Validation Loss: 1.310\n",
      "Saved checkpoint.\n",
      "Iteration: 16169\n",
      "Epoch 24 --- Training Accuracy:  65.6%, Validation Accuracy:  69.8%, Train Loss: 1.288 Validation Loss: 1.281\n",
      "Saved checkpoint.\n",
      "Iteration: 16872\n",
      "Epoch 25 --- Training Accuracy:  67.2%, Validation Accuracy:  69.9%, Train Loss: 1.383 Validation Loss: 1.289\n",
      "Saved checkpoint.\n",
      "Iteration: 17575\n",
      "Epoch 26 --- Training Accuracy:  68.8%, Validation Accuracy:  70.4%, Train Loss: 1.424 Validation Loss: 1.244\n",
      "Saved checkpoint.\n",
      "Iteration: 18278\n",
      "Epoch 27 --- Training Accuracy:  65.6%, Validation Accuracy:  67.5%, Train Loss: 1.276 Validation Loss: 1.316\n",
      "Iteration: 18981\n",
      "Epoch 28 --- Training Accuracy:  68.8%, Validation Accuracy:  72.4%, Train Loss: 1.278 Validation Loss: 1.167\n",
      "Saved checkpoint.\n",
      "Iteration: 19684\n",
      "Epoch 29 --- Training Accuracy:  60.9%, Validation Accuracy:  68.6%, Train Loss: 1.325 Validation Loss: 1.249\n",
      "Iteration: 20387\n",
      "Epoch 30 --- Training Accuracy:  65.6%, Validation Accuracy:  66.8%, Train Loss: 1.269 Validation Loss: 1.313\n",
      "Iteration: 21090\n",
      "Epoch 31 --- Training Accuracy:  70.3%, Validation Accuracy:  71.9%, Train Loss: 1.266 Validation Loss: 1.156\n",
      "Iteration: 21793\n",
      "Epoch 32 --- Training Accuracy:  81.2%, Validation Accuracy:  73.1%, Train Loss: 1.112 Validation Loss: 1.126\n",
      "Saved checkpoint.\n",
      "Iteration: 22496\n",
      "Epoch 33 --- Training Accuracy:  68.8%, Validation Accuracy:  72.0%, Train Loss: 1.207 Validation Loss: 1.136\n",
      "Iteration: 23199\n",
      "Epoch 34 --- Training Accuracy:  71.9%, Validation Accuracy:  72.3%, Train Loss: 1.154 Validation Loss: 1.135\n",
      "Iteration: 23902\n",
      "Epoch 35 --- Training Accuracy:  70.3%, Validation Accuracy:  71.6%, Train Loss: 1.156 Validation Loss: 1.144\n",
      "Iteration: 24605\n",
      "Epoch 36 --- Training Accuracy:  79.7%, Validation Accuracy:  72.0%, Train Loss: 1.138 Validation Loss: 1.131\n",
      "Iteration: 25308\n",
      "Epoch 37 --- Training Accuracy:  65.6%, Validation Accuracy:  70.5%, Train Loss: 1.340 Validation Loss: 1.164\n",
      "Iteration: 26011\n",
      "Epoch 38 --- Training Accuracy:  79.7%, Validation Accuracy:  72.3%, Train Loss: 1.198 Validation Loss: 1.124\n",
      "Iteration: 26714\n",
      "Epoch 39 --- Training Accuracy:  79.7%, Validation Accuracy:  70.4%, Train Loss: 0.981 Validation Loss: 1.190\n",
      "Iteration: 27417\n",
      "Epoch 40 --- Training Accuracy:  76.6%, Validation Accuracy:  72.7%, Train Loss: 0.975 Validation Loss: 1.100\n",
      "Iteration: 28120\n",
      "Epoch 41 --- Training Accuracy:  59.4%, Validation Accuracy:  71.6%, Train Loss: 1.347 Validation Loss: 1.125\n",
      "Iteration: 28823\n",
      "Epoch 42 --- Training Accuracy:  81.2%, Validation Accuracy:  72.2%, Train Loss: 1.028 Validation Loss: 1.107\n",
      "Iteration: 29526\n",
      "Epoch 43 --- Training Accuracy:  68.8%, Validation Accuracy:  71.8%, Train Loss: 1.195 Validation Loss: 1.136\n",
      "Iteration: 30229\n",
      "Epoch 44 --- Training Accuracy:  75.0%, Validation Accuracy:  70.0%, Train Loss: 1.188 Validation Loss: 1.167\n",
      "Iteration: 30932\n",
      "Epoch 45 --- Training Accuracy:  73.4%, Validation Accuracy:  69.9%, Train Loss: 1.106 Validation Loss: 1.162\n",
      "Iteration: 31635\n",
      "Epoch 46 --- Training Accuracy:  82.8%, Validation Accuracy:  73.8%, Train Loss: 0.928 Validation Loss: 1.069\n",
      "Saved checkpoint.\n",
      "Iteration: 32338\n",
      "Epoch 47 --- Training Accuracy:  71.9%, Validation Accuracy:  70.9%, Train Loss: 1.310 Validation Loss: 1.182\n",
      "Iteration: 33041\n",
      "Epoch 48 --- Training Accuracy:  75.0%, Validation Accuracy:  75.3%, Train Loss: 1.028 Validation Loss: 1.027\n",
      "Saved checkpoint.\n",
      "Iteration: 33744\n",
      "Epoch 49 --- Training Accuracy:  81.2%, Validation Accuracy:  70.7%, Train Loss: 0.885 Validation Loss: 1.158\n",
      "Iteration: 34447\n",
      "Epoch 50 --- Training Accuracy:  70.3%, Validation Accuracy:  72.8%, Train Loss: 1.155 Validation Loss: 1.091\n",
      "Iteration: 35150\n",
      "Epoch 51 --- Training Accuracy:  71.9%, Validation Accuracy:  74.8%, Train Loss: 1.070 Validation Loss: 1.043\n",
      "Iteration: 35853\n",
      "Epoch 52 --- Training Accuracy:  73.4%, Validation Accuracy:  72.2%, Train Loss: 0.992 Validation Loss: 1.112\n",
      "Iteration: 36556\n",
      "Epoch 53 --- Training Accuracy:  67.2%, Validation Accuracy:  75.7%, Train Loss: 1.146 Validation Loss: 1.026\n",
      "Saved checkpoint.\n",
      "Iteration: 37259\n",
      "Epoch 54 --- Training Accuracy:  73.4%, Validation Accuracy:  73.0%, Train Loss: 1.189 Validation Loss: 1.106\n",
      "Iteration: 37962\n",
      "Epoch 55 --- Training Accuracy:  71.9%, Validation Accuracy:  74.5%, Train Loss: 1.087 Validation Loss: 1.060\n",
      "Iteration: 38665\n",
      "Epoch 56 --- Training Accuracy:  87.5%, Validation Accuracy:  70.1%, Train Loss: 0.862 Validation Loss: 1.201\n",
      "Iteration: 39368\n",
      "Epoch 57 --- Training Accuracy:  71.9%, Validation Accuracy:  75.0%, Train Loss: 1.189 Validation Loss: 1.039\n",
      "Iteration: 40071\n",
      "Epoch 58 --- Training Accuracy:  78.1%, Validation Accuracy:  74.3%, Train Loss: 1.107 Validation Loss: 1.054\n",
      "Iteration: 40774\n",
      "Epoch 59 --- Training Accuracy:  68.8%, Validation Accuracy:  75.6%, Train Loss: 1.131 Validation Loss: 1.028\n",
      "Iteration: 41477\n",
      "Epoch 60 --- Training Accuracy:  82.8%, Validation Accuracy:  75.4%, Train Loss: 0.951 Validation Loss: 1.031\n",
      "Iteration: 42180\n",
      "Epoch 61 --- Training Accuracy:  78.1%, Validation Accuracy:  75.4%, Train Loss: 0.991 Validation Loss: 1.040\n",
      "Iteration: 42883\n",
      "Epoch 62 --- Training Accuracy:  64.1%, Validation Accuracy:  72.4%, Train Loss: 1.252 Validation Loss: 1.135\n",
      "Iteration: 43586\n",
      "Epoch 63 --- Training Accuracy:  73.4%, Validation Accuracy:  75.8%, Train Loss: 1.130 Validation Loss: 1.013\n",
      "Saved checkpoint.\n",
      "Iteration: 44289\n",
      "Epoch 64 --- Training Accuracy:  73.4%, Validation Accuracy:  73.4%, Train Loss: 0.999 Validation Loss: 1.094\n",
      "Iteration: 44992\n",
      "Epoch 65 --- Training Accuracy:  73.4%, Validation Accuracy:  73.9%, Train Loss: 1.008 Validation Loss: 1.061\n",
      "Iteration: 45695\n",
      "Epoch 66 --- Training Accuracy:  79.7%, Validation Accuracy:  75.0%, Train Loss: 0.977 Validation Loss: 1.052\n",
      "Iteration: 46398\n",
      "Epoch 67 --- Training Accuracy:  76.6%, Validation Accuracy:  76.2%, Train Loss: 1.081 Validation Loss: 1.015\n",
      "Saved checkpoint.\n",
      "Iteration: 47101\n",
      "Epoch 68 --- Training Accuracy:  76.6%, Validation Accuracy:  76.1%, Train Loss: 1.050 Validation Loss: 1.026\n",
      "Iteration: 47804\n",
      "Epoch 69 --- Training Accuracy:  65.6%, Validation Accuracy:  74.2%, Train Loss: 1.182 Validation Loss: 1.056\n",
      "Iteration: 48507\n",
      "Epoch 70 --- Training Accuracy:  73.4%, Validation Accuracy:  74.5%, Train Loss: 1.021 Validation Loss: 1.067\n",
      "Iteration: 49210\n",
      "Epoch 71 --- Training Accuracy:  79.7%, Validation Accuracy:  71.7%, Train Loss: 1.087 Validation Loss: 1.162\n",
      "Iteration: 49913\n",
      "Epoch 72 --- Training Accuracy:  71.9%, Validation Accuracy:  75.1%, Train Loss: 1.071 Validation Loss: 1.050\n",
      "Iteration: 50616\n",
      "Epoch 73 --- Training Accuracy:  70.3%, Validation Accuracy:  75.1%, Train Loss: 1.266 Validation Loss: 1.042\n",
      "Iteration: 51319\n",
      "Epoch 74 --- Training Accuracy:  79.7%, Validation Accuracy:  74.7%, Train Loss: 0.965 Validation Loss: 1.041\n",
      "Iteration: 52022\n",
      "Epoch 75 --- Training Accuracy:  71.9%, Validation Accuracy:  73.8%, Train Loss: 0.986 Validation Loss: 1.085\n",
      "Iteration: 52725\n",
      "Epoch 76 --- Training Accuracy:  73.4%, Validation Accuracy:  74.7%, Train Loss: 1.010 Validation Loss: 1.047\n",
      "Iteration: 53428\n",
      "Epoch 77 --- Training Accuracy:  70.3%, Validation Accuracy:  75.3%, Train Loss: 1.206 Validation Loss: 1.046\n",
      "Iteration: 54131\n",
      "Epoch 78 --- Training Accuracy:  82.8%, Validation Accuracy:  75.6%, Train Loss: 0.886 Validation Loss: 1.037\n",
      "Iteration: 54834\n",
      "Epoch 79 --- Training Accuracy:  70.3%, Validation Accuracy:  75.1%, Train Loss: 1.105 Validation Loss: 1.033\n",
      "Iteration: 55537\n",
      "Epoch 80 --- Training Accuracy:  68.8%, Validation Accuracy:  72.3%, Train Loss: 1.037 Validation Loss: 1.127\n",
      "Iteration: 56240\n",
      "Epoch 81 --- Training Accuracy:  75.0%, Validation Accuracy:  74.7%, Train Loss: 1.048 Validation Loss: 1.037\n",
      "Iteration: 56943\n",
      "Epoch 82 --- Training Accuracy:  82.8%, Validation Accuracy:  75.7%, Train Loss: 0.847 Validation Loss: 1.026\n",
      "Iteration: 57646\n",
      "Epoch 83 --- Training Accuracy:  75.0%, Validation Accuracy:  72.4%, Train Loss: 1.119 Validation Loss: 1.123\n",
      "Iteration: 58349\n",
      "Epoch 84 --- Training Accuracy:  82.8%, Validation Accuracy:  74.8%, Train Loss: 0.936 Validation Loss: 1.060\n",
      "Iteration: 59052\n",
      "Epoch 85 --- Training Accuracy:  73.4%, Validation Accuracy:  74.5%, Train Loss: 1.034 Validation Loss: 1.083\n",
      "Iteration: 59755\n",
      "Epoch 86 --- Training Accuracy:  70.3%, Validation Accuracy:  76.1%, Train Loss: 1.189 Validation Loss: 1.019\n",
      "Iteration: 60458\n",
      "Epoch 87 --- Training Accuracy:  84.4%, Validation Accuracy:  75.8%, Train Loss: 0.917 Validation Loss: 1.002\n",
      "Iteration: 61161\n",
      "Epoch 88 --- Training Accuracy:  73.4%, Validation Accuracy:  76.9%, Train Loss: 1.015 Validation Loss: 0.991\n",
      "Saved checkpoint.\n",
      "Iteration: 61864\n",
      "Epoch 89 --- Training Accuracy:  76.6%, Validation Accuracy:  73.4%, Train Loss: 1.014 Validation Loss: 1.107\n",
      "Iteration: 62567\n",
      "Epoch 90 --- Training Accuracy:  78.1%, Validation Accuracy:  73.6%, Train Loss: 1.096 Validation Loss: 1.096\n",
      "Iteration: 63270\n",
      "Epoch 91 --- Training Accuracy:  82.8%, Validation Accuracy:  77.1%, Train Loss: 0.930 Validation Loss: 0.978\n",
      "Saved checkpoint.\n",
      "Iteration: 63973\n",
      "Epoch 92 --- Training Accuracy:  87.5%, Validation Accuracy:  75.9%, Train Loss: 0.835 Validation Loss: 1.017\n",
      "Iteration: 64676\n",
      "Epoch 93 --- Training Accuracy:  73.4%, Validation Accuracy:  75.8%, Train Loss: 1.156 Validation Loss: 1.026\n",
      "Iteration: 65379\n",
      "Epoch 94 --- Training Accuracy:  75.0%, Validation Accuracy:  74.5%, Train Loss: 0.994 Validation Loss: 1.084\n",
      "Iteration: 66082\n",
      "Epoch 95 --- Training Accuracy:  65.6%, Validation Accuracy:  77.3%, Train Loss: 1.198 Validation Loss: 0.988\n",
      "Saved checkpoint.\n",
      "Iteration: 66785\n",
      "Epoch 96 --- Training Accuracy:  76.6%, Validation Accuracy:  76.8%, Train Loss: 1.111 Validation Loss: 1.019\n",
      "Iteration: 67488\n",
      "Epoch 97 --- Training Accuracy:  68.8%, Validation Accuracy:  75.7%, Train Loss: 1.113 Validation Loss: 1.059\n",
      "Iteration: 68191\n",
      "Epoch 98 --- Training Accuracy:  71.9%, Validation Accuracy:  71.0%, Train Loss: 0.994 Validation Loss: 1.174\n",
      "Iteration: 68894\n",
      "Epoch 99 --- Training Accuracy:  76.6%, Validation Accuracy:  77.4%, Train Loss: 0.977 Validation Loss: 0.964\n",
      "Saved checkpoint.\n",
      "Iteration: 69597\n",
      "Epoch 100 --- Training Accuracy:  82.8%, Validation Accuracy:  76.0%, Train Loss: 0.889 Validation Loss: 1.014\n",
      "Iteration: 70300\n",
      "Epoch 101 --- Training Accuracy:  89.1%, Validation Accuracy:  75.2%, Train Loss: 0.847 Validation Loss: 1.028\n",
      "Iteration: 71003\n",
      "Epoch 102 --- Training Accuracy:  76.6%, Validation Accuracy:  73.2%, Train Loss: 1.014 Validation Loss: 1.109\n",
      "Iteration: 71706\n",
      "Epoch 103 --- Training Accuracy:  82.8%, Validation Accuracy:  77.4%, Train Loss: 0.823 Validation Loss: 0.989\n",
      "Iteration: 72409\n",
      "Epoch 104 --- Training Accuracy:  75.0%, Validation Accuracy:  76.2%, Train Loss: 0.968 Validation Loss: 1.020\n",
      "Iteration: 73112\n",
      "Epoch 105 --- Training Accuracy:  81.2%, Validation Accuracy:  76.8%, Train Loss: 0.874 Validation Loss: 0.988\n",
      "Iteration: 73815\n",
      "Epoch 106 --- Training Accuracy:  70.3%, Validation Accuracy:  75.5%, Train Loss: 1.261 Validation Loss: 1.046\n",
      "Iteration: 74518\n",
      "Epoch 107 --- Training Accuracy:  65.6%, Validation Accuracy:  76.0%, Train Loss: 1.152 Validation Loss: 1.026\n",
      "Iteration: 75221\n",
      "Epoch 108 --- Training Accuracy:  79.7%, Validation Accuracy:  77.8%, Train Loss: 0.922 Validation Loss: 0.990\n",
      "Saved checkpoint.\n",
      "Iteration: 75924\n",
      "Epoch 109 --- Training Accuracy:  78.1%, Validation Accuracy:  74.4%, Train Loss: 1.026 Validation Loss: 1.081\n",
      "Iteration: 76627\n",
      "Epoch 110 --- Training Accuracy:  79.7%, Validation Accuracy:  76.9%, Train Loss: 0.905 Validation Loss: 0.996\n",
      "Iteration: 77330\n",
      "Epoch 111 --- Training Accuracy:  64.1%, Validation Accuracy:  73.0%, Train Loss: 1.183 Validation Loss: 1.129\n",
      "Iteration: 78033\n",
      "Epoch 112 --- Training Accuracy:  70.3%, Validation Accuracy:  77.4%, Train Loss: 1.080 Validation Loss: 0.985\n",
      "Iteration: 78736\n",
      "Epoch 113 --- Training Accuracy:  84.4%, Validation Accuracy:  75.2%, Train Loss: 0.922 Validation Loss: 1.036\n",
      "Iteration: 79439\n",
      "Epoch 114 --- Training Accuracy:  84.4%, Validation Accuracy:  76.7%, Train Loss: 0.886 Validation Loss: 1.008\n",
      "Iteration: 80142\n",
      "Epoch 115 --- Training Accuracy:  78.1%, Validation Accuracy:  78.0%, Train Loss: 0.969 Validation Loss: 0.974\n",
      "Saved checkpoint.\n",
      "Iteration: 80845\n",
      "Epoch 116 --- Training Accuracy:  82.8%, Validation Accuracy:  76.1%, Train Loss: 0.952 Validation Loss: 1.018\n",
      "Iteration: 81548\n",
      "Epoch 117 --- Training Accuracy:  71.9%, Validation Accuracy:  77.4%, Train Loss: 1.111 Validation Loss: 0.982\n",
      "Iteration: 82251\n",
      "Epoch 118 --- Training Accuracy:  84.4%, Validation Accuracy:  76.4%, Train Loss: 0.838 Validation Loss: 1.012\n",
      "Iteration: 82954\n",
      "Epoch 119 --- Training Accuracy:  73.4%, Validation Accuracy:  76.2%, Train Loss: 1.095 Validation Loss: 1.002\n",
      "Iteration: 83657\n",
      "Epoch 120 --- Training Accuracy:  79.7%, Validation Accuracy:  77.7%, Train Loss: 1.009 Validation Loss: 0.991\n",
      "Iteration: 84360\n",
      "Epoch 121 --- Training Accuracy:  75.0%, Validation Accuracy:  77.3%, Train Loss: 1.060 Validation Loss: 0.989\n",
      "Iteration: 85063\n",
      "Epoch 122 --- Training Accuracy:  75.0%, Validation Accuracy:  76.6%, Train Loss: 1.080 Validation Loss: 0.998\n",
      "Iteration: 85766\n",
      "Epoch 123 --- Training Accuracy:  73.4%, Validation Accuracy:  77.2%, Train Loss: 1.094 Validation Loss: 0.991\n",
      "Iteration: 86469\n",
      "Epoch 124 --- Training Accuracy:  82.8%, Validation Accuracy:  77.6%, Train Loss: 0.855 Validation Loss: 0.979\n",
      "Iteration: 87172\n",
      "Epoch 125 --- Training Accuracy:  75.0%, Validation Accuracy:  76.5%, Train Loss: 1.028 Validation Loss: 1.024\n",
      "Iteration: 87875\n",
      "Epoch 126 --- Training Accuracy:  78.1%, Validation Accuracy:  76.7%, Train Loss: 0.897 Validation Loss: 0.998\n",
      "Iteration: 88578\n",
      "Epoch 127 --- Training Accuracy:  81.2%, Validation Accuracy:  76.2%, Train Loss: 0.862 Validation Loss: 1.013\n",
      "Iteration: 89281\n",
      "Epoch 128 --- Training Accuracy:  78.1%, Validation Accuracy:  72.5%, Train Loss: 0.914 Validation Loss: 1.106\n",
      "Iteration: 89984\n",
      "Epoch 129 --- Training Accuracy:  79.7%, Validation Accuracy:  78.3%, Train Loss: 0.939 Validation Loss: 0.972\n",
      "Saved checkpoint.\n",
      "Iteration: 90687\n",
      "Epoch 130 --- Training Accuracy:  84.4%, Validation Accuracy:  75.6%, Train Loss: 1.017 Validation Loss: 1.065\n",
      "Iteration: 91390\n",
      "Epoch 131 --- Training Accuracy:  81.2%, Validation Accuracy:  76.7%, Train Loss: 0.826 Validation Loss: 0.996\n",
      "Iteration: 92093\n",
      "Epoch 132 --- Training Accuracy:  81.2%, Validation Accuracy:  77.0%, Train Loss: 0.905 Validation Loss: 0.988\n",
      "Iteration: 92796\n",
      "Epoch 133 --- Training Accuracy:  73.4%, Validation Accuracy:  76.5%, Train Loss: 1.092 Validation Loss: 1.020\n",
      "Iteration: 93499\n",
      "Epoch 134 --- Training Accuracy:  82.8%, Validation Accuracy:  77.7%, Train Loss: 0.980 Validation Loss: 0.986\n",
      "Iteration: 94202\n",
      "Epoch 135 --- Training Accuracy:  87.5%, Validation Accuracy:  77.6%, Train Loss: 0.751 Validation Loss: 0.980\n",
      "Iteration: 94905\n",
      "Epoch 136 --- Training Accuracy:  68.8%, Validation Accuracy:  78.7%, Train Loss: 0.956 Validation Loss: 0.970\n",
      "Saved checkpoint.\n",
      "Iteration: 95608\n",
      "Epoch 137 --- Training Accuracy:  75.0%, Validation Accuracy:  77.9%, Train Loss: 0.960 Validation Loss: 0.985\n",
      "Iteration: 96311\n",
      "Epoch 138 --- Training Accuracy:  87.5%, Validation Accuracy:  77.9%, Train Loss: 0.858 Validation Loss: 0.973\n",
      "Iteration: 97014\n",
      "Epoch 139 --- Training Accuracy:  78.1%, Validation Accuracy:  78.4%, Train Loss: 0.905 Validation Loss: 0.969\n",
      "Iteration: 97717\n",
      "Epoch 140 --- Training Accuracy:  82.8%, Validation Accuracy:  75.9%, Train Loss: 0.945 Validation Loss: 1.034\n",
      "Iteration: 98420\n",
      "Epoch 141 --- Training Accuracy:  79.7%, Validation Accuracy:  77.2%, Train Loss: 0.956 Validation Loss: 0.987\n",
      "Iteration: 99123\n",
      "Epoch 142 --- Training Accuracy:  84.4%, Validation Accuracy:  79.0%, Train Loss: 0.809 Validation Loss: 0.951\n",
      "Saved checkpoint.\n",
      "Iteration: 99826\n",
      "Epoch 143 --- Training Accuracy:  73.4%, Validation Accuracy:  76.6%, Train Loss: 1.037 Validation Loss: 1.014\n",
      "Iteration: 100529\n",
      "Epoch 144 --- Training Accuracy:  79.7%, Validation Accuracy:  78.2%, Train Loss: 0.962 Validation Loss: 0.972\n",
      "Iteration: 101232\n",
      "Epoch 145 --- Training Accuracy:  81.2%, Validation Accuracy:  76.6%, Train Loss: 0.921 Validation Loss: 1.031\n",
      "Iteration: 101935\n",
      "Epoch 146 --- Training Accuracy:  75.0%, Validation Accuracy:  75.8%, Train Loss: 1.036 Validation Loss: 1.034\n",
      "Iteration: 102638\n",
      "Epoch 147 --- Training Accuracy:  81.2%, Validation Accuracy:  76.1%, Train Loss: 0.842 Validation Loss: 1.030\n",
      "Iteration: 103341\n",
      "Epoch 148 --- Training Accuracy:  84.4%, Validation Accuracy:  78.9%, Train Loss: 0.913 Validation Loss: 0.947\n",
      "Iteration: 104044\n",
      "Epoch 149 --- Training Accuracy:  82.8%, Validation Accuracy:  77.0%, Train Loss: 0.842 Validation Loss: 1.000\n",
      "Iteration: 104747\n",
      "Epoch 150 --- Training Accuracy:  75.0%, Validation Accuracy:  78.9%, Train Loss: 1.044 Validation Loss: 0.940\n",
      "Iteration: 105450\n",
      "Epoch 151 --- Training Accuracy:  79.7%, Validation Accuracy:  78.4%, Train Loss: 0.926 Validation Loss: 0.964\n",
      "Iteration: 106153\n",
      "Epoch 152 --- Training Accuracy:  92.2%, Validation Accuracy:  78.2%, Train Loss: 0.694 Validation Loss: 0.963\n",
      "Iteration: 106856\n",
      "Epoch 153 --- Training Accuracy:  73.4%, Validation Accuracy:  75.8%, Train Loss: 0.982 Validation Loss: 1.041\n",
      "Iteration: 107559\n",
      "Epoch 154 --- Training Accuracy:  73.4%, Validation Accuracy:  77.3%, Train Loss: 0.978 Validation Loss: 0.996\n",
      "Iteration: 108262\n",
      "Epoch 155 --- Training Accuracy:  81.2%, Validation Accuracy:  77.7%, Train Loss: 0.818 Validation Loss: 0.984\n",
      "Iteration: 108965\n",
      "Epoch 156 --- Training Accuracy:  87.5%, Validation Accuracy:  78.0%, Train Loss: 0.814 Validation Loss: 0.973\n",
      "Iteration: 109668\n",
      "Epoch 157 --- Training Accuracy:  76.6%, Validation Accuracy:  72.7%, Train Loss: 1.003 Validation Loss: 1.126\n",
      "Iteration: 110371\n",
      "Epoch 158 --- Training Accuracy:  78.1%, Validation Accuracy:  79.4%, Train Loss: 0.978 Validation Loss: 0.947\n",
      "Saved checkpoint.\n",
      "Iteration: 111074\n",
      "Epoch 159 --- Training Accuracy:  85.9%, Validation Accuracy:  77.6%, Train Loss: 0.814 Validation Loss: 0.973\n",
      "Iteration: 111777\n",
      "Epoch 160 --- Training Accuracy:  85.9%, Validation Accuracy:  77.5%, Train Loss: 0.844 Validation Loss: 0.983\n",
      "Iteration: 112480\n",
      "Epoch 161 --- Training Accuracy:  78.1%, Validation Accuracy:  76.7%, Train Loss: 1.063 Validation Loss: 0.999\n",
      "Iteration: 113183\n",
      "Epoch 162 --- Training Accuracy:  76.6%, Validation Accuracy:  78.3%, Train Loss: 0.965 Validation Loss: 0.966\n",
      "Iteration: 113886\n",
      "Epoch 163 --- Training Accuracy:  85.9%, Validation Accuracy:  76.9%, Train Loss: 0.862 Validation Loss: 1.004\n",
      "Iteration: 114589\n",
      "Epoch 164 --- Training Accuracy:  79.7%, Validation Accuracy:  79.3%, Train Loss: 0.934 Validation Loss: 0.946\n",
      "Iteration: 115292\n",
      "Epoch 165 --- Training Accuracy:  82.8%, Validation Accuracy:  77.4%, Train Loss: 0.856 Validation Loss: 0.987\n",
      "Iteration: 115995\n",
      "Epoch 166 --- Training Accuracy:  73.4%, Validation Accuracy:  78.0%, Train Loss: 1.011 Validation Loss: 0.978\n",
      "Iteration: 116698\n",
      "Epoch 167 --- Training Accuracy:  64.1%, Validation Accuracy:  78.5%, Train Loss: 1.288 Validation Loss: 0.980\n",
      "Iteration: 117401\n",
      "Epoch 168 --- Training Accuracy:  73.4%, Validation Accuracy:  78.3%, Train Loss: 1.034 Validation Loss: 0.973\n",
      "Iteration: 118104\n",
      "Epoch 169 --- Training Accuracy:  84.4%, Validation Accuracy:  76.2%, Train Loss: 0.904 Validation Loss: 1.031\n",
      "Iteration: 118807\n",
      "Epoch 170 --- Training Accuracy:  81.2%, Validation Accuracy:  78.0%, Train Loss: 0.881 Validation Loss: 0.974\n",
      "Iteration: 119510\n",
      "Epoch 171 --- Training Accuracy:  76.6%, Validation Accuracy:  77.7%, Train Loss: 0.976 Validation Loss: 0.994\n",
      "Iteration: 120213\n",
      "Epoch 172 --- Training Accuracy:  75.0%, Validation Accuracy:  72.6%, Train Loss: 1.003 Validation Loss: 1.117\n",
      "Iteration: 120916\n",
      "Epoch 173 --- Training Accuracy:  82.8%, Validation Accuracy:  77.2%, Train Loss: 0.938 Validation Loss: 0.994\n",
      "Iteration: 121619\n",
      "Epoch 174 --- Training Accuracy:  84.4%, Validation Accuracy:  77.4%, Train Loss: 0.827 Validation Loss: 0.993\n",
      "Iteration: 122322\n",
      "Epoch 175 --- Training Accuracy:  76.6%, Validation Accuracy:  78.2%, Train Loss: 0.946 Validation Loss: 0.965\n",
      "Iteration: 123025\n",
      "Epoch 176 --- Training Accuracy:  79.7%, Validation Accuracy:  76.0%, Train Loss: 0.867 Validation Loss: 1.021\n",
      "Iteration: 123728\n",
      "Epoch 177 --- Training Accuracy:  68.8%, Validation Accuracy:  76.9%, Train Loss: 1.140 Validation Loss: 1.008\n",
      "Iteration: 124431\n",
      "Epoch 178 --- Training Accuracy:  79.7%, Validation Accuracy:  78.9%, Train Loss: 0.983 Validation Loss: 0.943\n",
      "Iteration: 125134\n",
      "Epoch 179 --- Training Accuracy:  82.8%, Validation Accuracy:  78.7%, Train Loss: 0.796 Validation Loss: 0.962\n",
      "Iteration: 125837\n",
      "Epoch 180 --- Training Accuracy:  85.9%, Validation Accuracy:  75.2%, Train Loss: 0.884 Validation Loss: 1.048\n",
      "Iteration: 126540\n",
      "Epoch 181 --- Training Accuracy:  82.8%, Validation Accuracy:  78.0%, Train Loss: 0.880 Validation Loss: 0.975\n",
      "Iteration: 127243\n",
      "Epoch 182 --- Training Accuracy:  81.2%, Validation Accuracy:  79.1%, Train Loss: 1.016 Validation Loss: 0.937\n",
      "Iteration: 127946\n",
      "Epoch 183 --- Training Accuracy:  85.9%, Validation Accuracy:  78.8%, Train Loss: 0.807 Validation Loss: 0.944\n",
      "Iteration: 128649\n",
      "Epoch 184 --- Training Accuracy:  85.9%, Validation Accuracy:  78.3%, Train Loss: 0.738 Validation Loss: 0.958\n",
      "Iteration: 129352\n",
      "Epoch 185 --- Training Accuracy:  85.9%, Validation Accuracy:  79.6%, Train Loss: 0.816 Validation Loss: 0.935\n",
      "Saved checkpoint.\n",
      "Iteration: 130055\n",
      "Epoch 186 --- Training Accuracy:  87.5%, Validation Accuracy:  78.7%, Train Loss: 0.734 Validation Loss: 0.948\n",
      "Iteration: 130758\n",
      "Epoch 187 --- Training Accuracy:  78.1%, Validation Accuracy:  78.5%, Train Loss: 0.868 Validation Loss: 0.957\n",
      "Iteration: 131461\n",
      "Epoch 188 --- Training Accuracy:  76.6%, Validation Accuracy:  77.9%, Train Loss: 1.039 Validation Loss: 0.980\n",
      "Iteration: 132164\n",
      "Epoch 189 --- Training Accuracy:  78.1%, Validation Accuracy:  78.8%, Train Loss: 1.153 Validation Loss: 0.945\n",
      "Iteration: 132867\n",
      "Epoch 190 --- Training Accuracy:  87.5%, Validation Accuracy:  77.9%, Train Loss: 0.796 Validation Loss: 0.984\n",
      "Iteration: 133570\n",
      "Epoch 191 --- Training Accuracy:  73.4%, Validation Accuracy:  78.7%, Train Loss: 0.961 Validation Loss: 0.970\n",
      "Iteration: 134273\n",
      "Epoch 192 --- Training Accuracy:  79.7%, Validation Accuracy:  78.4%, Train Loss: 0.923 Validation Loss: 0.966\n",
      "Iteration: 134976\n",
      "Epoch 193 --- Training Accuracy:  76.6%, Validation Accuracy:  77.8%, Train Loss: 1.027 Validation Loss: 0.981\n",
      "Iteration: 135679\n",
      "Epoch 194 --- Training Accuracy:  81.2%, Validation Accuracy:  78.7%, Train Loss: 0.886 Validation Loss: 0.943\n",
      "Iteration: 136382\n",
      "Epoch 195 --- Training Accuracy:  82.8%, Validation Accuracy:  78.4%, Train Loss: 0.902 Validation Loss: 0.975\n",
      "Iteration: 137085\n",
      "Epoch 196 --- Training Accuracy:  89.1%, Validation Accuracy:  77.3%, Train Loss: 0.786 Validation Loss: 1.007\n",
      "Iteration: 137788\n",
      "Epoch 197 --- Training Accuracy:  87.5%, Validation Accuracy:  78.7%, Train Loss: 0.774 Validation Loss: 0.963\n",
      "Iteration: 138491\n",
      "Epoch 198 --- Training Accuracy:  81.2%, Validation Accuracy:  78.3%, Train Loss: 0.913 Validation Loss: 0.977\n",
      "Iteration: 139194\n",
      "Epoch 199 --- Training Accuracy:  87.5%, Validation Accuracy:  78.8%, Train Loss: 0.696 Validation Loss: 0.956\n",
      "Iteration: 139897\n",
      "Epoch 200 --- Training Accuracy:  84.4%, Validation Accuracy:  78.5%, Train Loss: 0.876 Validation Loss: 0.988\n",
      "Iteration: 140600\n",
      "Epoch 201 --- Training Accuracy:  78.1%, Validation Accuracy:  78.2%, Train Loss: 0.862 Validation Loss: 0.980\n",
      "Iteration: 141303\n",
      "Epoch 202 --- Training Accuracy:  84.4%, Validation Accuracy:  78.5%, Train Loss: 0.748 Validation Loss: 0.974\n",
      "Iteration: 142006\n",
      "Epoch 203 --- Training Accuracy:  82.8%, Validation Accuracy:  79.6%, Train Loss: 0.901 Validation Loss: 0.929\n",
      "Iteration: 142709\n",
      "Epoch 204 --- Training Accuracy:  81.2%, Validation Accuracy:  78.5%, Train Loss: 0.914 Validation Loss: 0.971\n",
      "Iteration: 143412\n",
      "Epoch 205 --- Training Accuracy:  85.9%, Validation Accuracy:  73.5%, Train Loss: 0.786 Validation Loss: 1.144\n",
      "Iteration: 144115\n",
      "Epoch 206 --- Training Accuracy:  87.5%, Validation Accuracy:  78.1%, Train Loss: 0.761 Validation Loss: 0.973\n",
      "Iteration: 144818\n",
      "Epoch 207 --- Training Accuracy:  73.4%, Validation Accuracy:  78.9%, Train Loss: 0.971 Validation Loss: 0.945\n",
      "Iteration: 145521\n",
      "Epoch 208 --- Training Accuracy:  84.4%, Validation Accuracy:  78.7%, Train Loss: 0.834 Validation Loss: 0.964\n",
      "Iteration: 146224\n",
      "Epoch 209 --- Training Accuracy:  79.7%, Validation Accuracy:  78.0%, Train Loss: 0.910 Validation Loss: 0.965\n",
      "Iteration: 146927\n",
      "Epoch 210 --- Training Accuracy:  78.1%, Validation Accuracy:  78.4%, Train Loss: 0.922 Validation Loss: 0.968\n",
      "Iteration: 147630\n",
      "Epoch 211 --- Training Accuracy:  79.7%, Validation Accuracy:  78.9%, Train Loss: 0.922 Validation Loss: 0.974\n",
      "Iteration: 148333\n",
      "Epoch 212 --- Training Accuracy:  82.8%, Validation Accuracy:  79.0%, Train Loss: 0.830 Validation Loss: 0.947\n",
      "Iteration: 149036\n",
      "Epoch 213 --- Training Accuracy:  78.1%, Validation Accuracy:  75.9%, Train Loss: 0.924 Validation Loss: 1.059\n",
      "Iteration: 149739\n",
      "Epoch 214 --- Training Accuracy:  78.1%, Validation Accuracy:  79.1%, Train Loss: 0.881 Validation Loss: 0.943\n",
      "Iteration: 150442\n",
      "Epoch 215 --- Training Accuracy:  81.2%, Validation Accuracy:  78.7%, Train Loss: 0.898 Validation Loss: 0.972\n",
      "Iteration: 151145\n",
      "Epoch 216 --- Training Accuracy:  85.9%, Validation Accuracy:  78.6%, Train Loss: 0.864 Validation Loss: 0.954\n",
      "Iteration: 151848\n",
      "Epoch 217 --- Training Accuracy:  85.9%, Validation Accuracy:  78.4%, Train Loss: 0.856 Validation Loss: 0.974\n",
      "Iteration: 152551\n",
      "Epoch 218 --- Training Accuracy:  81.2%, Validation Accuracy:  79.3%, Train Loss: 0.847 Validation Loss: 0.940\n",
      "Iteration: 153254\n",
      "Epoch 219 --- Training Accuracy:  85.9%, Validation Accuracy:  78.9%, Train Loss: 0.815 Validation Loss: 0.943\n",
      "Iteration: 153957\n",
      "Epoch 220 --- Training Accuracy:  76.6%, Validation Accuracy:  78.2%, Train Loss: 0.897 Validation Loss: 0.963\n",
      "Iteration: 154660\n",
      "Epoch 221 --- Training Accuracy:  73.4%, Validation Accuracy:  75.9%, Train Loss: 1.042 Validation Loss: 1.049\n",
      "Iteration: 155363\n",
      "Epoch 222 --- Training Accuracy:  82.8%, Validation Accuracy:  79.1%, Train Loss: 0.953 Validation Loss: 0.940\n",
      "Iteration: 156066\n",
      "Epoch 223 --- Training Accuracy:  85.9%, Validation Accuracy:  76.2%, Train Loss: 0.809 Validation Loss: 1.040\n",
      "Iteration: 156769\n",
      "Epoch 224 --- Training Accuracy:  78.1%, Validation Accuracy:  78.0%, Train Loss: 0.997 Validation Loss: 0.957\n",
      "Iteration: 157472\n",
      "Epoch 225 --- Training Accuracy:  78.1%, Validation Accuracy:  76.3%, Train Loss: 1.091 Validation Loss: 1.040\n",
      "Iteration: 158175\n",
      "Epoch 226 --- Training Accuracy:  82.8%, Validation Accuracy:  79.3%, Train Loss: 0.876 Validation Loss: 0.947\n",
      "Iteration: 158878\n",
      "Epoch 227 --- Training Accuracy:  82.8%, Validation Accuracy:  78.9%, Train Loss: 0.908 Validation Loss: 0.943\n",
      "Iteration: 159581\n",
      "Epoch 228 --- Training Accuracy:  87.5%, Validation Accuracy:  79.1%, Train Loss: 0.784 Validation Loss: 0.945\n",
      "Iteration: 160284\n",
      "Epoch 229 --- Training Accuracy:  71.9%, Validation Accuracy:  78.6%, Train Loss: 1.069 Validation Loss: 0.972\n",
      "Iteration: 160987\n",
      "Epoch 230 --- Training Accuracy:  75.0%, Validation Accuracy:  79.1%, Train Loss: 0.907 Validation Loss: 0.941\n",
      "Iteration: 161690\n",
      "Epoch 231 --- Training Accuracy:  82.8%, Validation Accuracy:  80.0%, Train Loss: 0.822 Validation Loss: 0.927\n",
      "Saved checkpoint.\n",
      "Iteration: 162393\n",
      "Epoch 232 --- Training Accuracy:  85.9%, Validation Accuracy:  79.3%, Train Loss: 0.790 Validation Loss: 0.940\n",
      "Iteration: 163096\n",
      "Epoch 233 --- Training Accuracy:  92.2%, Validation Accuracy:  78.4%, Train Loss: 0.737 Validation Loss: 0.968\n",
      "Iteration: 163799\n",
      "Epoch 234 --- Training Accuracy:  84.4%, Validation Accuracy:  78.4%, Train Loss: 0.945 Validation Loss: 0.965\n",
      "Iteration: 164502\n",
      "Epoch 235 --- Training Accuracy:  84.4%, Validation Accuracy:  79.7%, Train Loss: 0.806 Validation Loss: 0.921\n",
      "Iteration: 165205\n",
      "Epoch 236 --- Training Accuracy:  81.2%, Validation Accuracy:  78.9%, Train Loss: 0.955 Validation Loss: 0.959\n",
      "Iteration: 165908\n",
      "Epoch 237 --- Training Accuracy:  73.4%, Validation Accuracy:  73.5%, Train Loss: 0.991 Validation Loss: 1.124\n",
      "Iteration: 166611\n",
      "Epoch 238 --- Training Accuracy:  70.3%, Validation Accuracy:  79.1%, Train Loss: 1.124 Validation Loss: 0.939\n",
      "Iteration: 167314\n",
      "Epoch 239 --- Training Accuracy:  92.2%, Validation Accuracy:  79.4%, Train Loss: 0.743 Validation Loss: 0.952\n",
      "Iteration: 168017\n",
      "Epoch 240 --- Training Accuracy:  76.6%, Validation Accuracy:  77.9%, Train Loss: 0.989 Validation Loss: 0.974\n",
      "Iteration: 168720\n",
      "Epoch 241 --- Training Accuracy:  78.1%, Validation Accuracy:  78.6%, Train Loss: 0.982 Validation Loss: 0.944\n",
      "Iteration: 169423\n",
      "Epoch 242 --- Training Accuracy:  85.9%, Validation Accuracy:  78.2%, Train Loss: 0.802 Validation Loss: 0.995\n",
      "Iteration: 170126\n",
      "Epoch 243 --- Training Accuracy:  89.1%, Validation Accuracy:  78.9%, Train Loss: 0.713 Validation Loss: 0.949\n",
      "Iteration: 170829\n",
      "Epoch 244 --- Training Accuracy:  81.2%, Validation Accuracy:  79.2%, Train Loss: 0.887 Validation Loss: 0.946\n",
      "Iteration: 171532\n",
      "Epoch 245 --- Training Accuracy:  75.0%, Validation Accuracy:  78.7%, Train Loss: 0.998 Validation Loss: 0.961\n",
      "Iteration: 172235\n",
      "Epoch 246 --- Training Accuracy:  73.4%, Validation Accuracy:  78.1%, Train Loss: 1.026 Validation Loss: 0.980\n",
      "Iteration: 172938\n",
      "Epoch 247 --- Training Accuracy:  75.0%, Validation Accuracy:  79.1%, Train Loss: 1.004 Validation Loss: 0.958\n",
      "Iteration: 173641\n",
      "Epoch 248 --- Training Accuracy:  78.1%, Validation Accuracy:  77.9%, Train Loss: 0.905 Validation Loss: 0.996\n",
      "Iteration: 174344\n",
      "Epoch 249 --- Training Accuracy:  81.2%, Validation Accuracy:  79.4%, Train Loss: 0.916 Validation Loss: 0.935\n",
      "Iteration: 175047\n",
      "Epoch 250 --- Training Accuracy:  87.5%, Validation Accuracy:  80.0%, Train Loss: 0.812 Validation Loss: 0.904\n",
      "Iteration: 175750\n",
      "Epoch 251 --- Training Accuracy:  73.4%, Validation Accuracy:  78.6%, Train Loss: 1.129 Validation Loss: 0.946\n",
      "Iteration: 176453\n",
      "Epoch 252 --- Training Accuracy:  76.6%, Validation Accuracy:  80.2%, Train Loss: 0.970 Validation Loss: 0.923\n",
      "Saved checkpoint.\n",
      "Iteration: 177156\n",
      "Epoch 253 --- Training Accuracy:  81.2%, Validation Accuracy:  79.3%, Train Loss: 0.827 Validation Loss: 0.944\n",
      "Iteration: 177859\n",
      "Epoch 254 --- Training Accuracy:  87.5%, Validation Accuracy:  77.8%, Train Loss: 0.822 Validation Loss: 0.976\n",
      "Iteration: 178562\n",
      "Epoch 255 --- Training Accuracy:  84.4%, Validation Accuracy:  80.2%, Train Loss: 0.840 Validation Loss: 0.923\n",
      "Iteration: 179265\n",
      "Epoch 256 --- Training Accuracy:  76.6%, Validation Accuracy:  79.5%, Train Loss: 1.059 Validation Loss: 0.936\n",
      "Iteration: 179968\n",
      "Epoch 257 --- Training Accuracy:  89.1%, Validation Accuracy:  79.9%, Train Loss: 0.690 Validation Loss: 0.927\n",
      "Iteration: 180671\n",
      "Epoch 258 --- Training Accuracy:  79.7%, Validation Accuracy:  79.5%, Train Loss: 0.907 Validation Loss: 0.939\n",
      "Iteration: 181374\n",
      "Epoch 259 --- Training Accuracy:  84.4%, Validation Accuracy:  80.6%, Train Loss: 0.972 Validation Loss: 0.906\n",
      "Saved checkpoint.\n",
      "Iteration: 182077\n",
      "Epoch 260 --- Training Accuracy:  79.7%, Validation Accuracy:  78.7%, Train Loss: 0.945 Validation Loss: 0.951\n",
      "Iteration: 182780\n",
      "Epoch 261 --- Training Accuracy:  87.5%, Validation Accuracy:  77.4%, Train Loss: 0.835 Validation Loss: 1.001\n",
      "Iteration: 183483\n",
      "Epoch 262 --- Training Accuracy:  85.9%, Validation Accuracy:  80.5%, Train Loss: 0.777 Validation Loss: 0.911\n",
      "Iteration: 184186\n",
      "Epoch 263 --- Training Accuracy:  90.6%, Validation Accuracy:  80.6%, Train Loss: 0.759 Validation Loss: 0.915\n",
      "Iteration: 184889\n",
      "Epoch 264 --- Training Accuracy:  75.0%, Validation Accuracy:  79.5%, Train Loss: 0.982 Validation Loss: 0.932\n",
      "Iteration: 185592\n",
      "Epoch 265 --- Training Accuracy:  79.7%, Validation Accuracy:  79.0%, Train Loss: 0.891 Validation Loss: 0.944\n",
      "Iteration: 186295\n",
      "Epoch 266 --- Training Accuracy:  82.8%, Validation Accuracy:  77.3%, Train Loss: 0.810 Validation Loss: 1.019\n",
      "Iteration: 186998\n",
      "Epoch 267 --- Training Accuracy:  85.9%, Validation Accuracy:  77.8%, Train Loss: 0.828 Validation Loss: 0.999\n",
      "Iteration: 187701\n",
      "Epoch 268 --- Training Accuracy:  81.2%, Validation Accuracy:  79.4%, Train Loss: 0.902 Validation Loss: 0.950\n",
      "Iteration: 188404\n",
      "Epoch 269 --- Training Accuracy:  82.8%, Validation Accuracy:  79.3%, Train Loss: 0.868 Validation Loss: 0.931\n",
      "Iteration: 189107\n",
      "Epoch 270 --- Training Accuracy:  82.8%, Validation Accuracy:  80.4%, Train Loss: 0.852 Validation Loss: 0.916\n",
      "Iteration: 189810\n",
      "Epoch 271 --- Training Accuracy:  81.2%, Validation Accuracy:  80.1%, Train Loss: 0.893 Validation Loss: 0.913\n",
      "Iteration: 190513\n",
      "Epoch 272 --- Training Accuracy:  79.7%, Validation Accuracy:  79.2%, Train Loss: 0.902 Validation Loss: 0.941\n",
      "Iteration: 191216\n",
      "Epoch 273 --- Training Accuracy:  85.9%, Validation Accuracy:  80.1%, Train Loss: 0.753 Validation Loss: 0.909\n",
      "Iteration: 191919\n",
      "Epoch 274 --- Training Accuracy:  85.9%, Validation Accuracy:  79.2%, Train Loss: 0.913 Validation Loss: 0.948\n",
      "Iteration: 192622\n",
      "Epoch 275 --- Training Accuracy:  81.2%, Validation Accuracy:  78.6%, Train Loss: 0.923 Validation Loss: 0.959\n",
      "Iteration: 193325\n",
      "Epoch 276 --- Training Accuracy:  78.1%, Validation Accuracy:  79.5%, Train Loss: 0.860 Validation Loss: 0.943\n",
      "Iteration: 194028\n",
      "Epoch 277 --- Training Accuracy:  79.7%, Validation Accuracy:  78.6%, Train Loss: 0.806 Validation Loss: 0.949\n",
      "Iteration: 194731\n",
      "Epoch 278 --- Training Accuracy:  82.8%, Validation Accuracy:  79.0%, Train Loss: 0.784 Validation Loss: 0.965\n",
      "Iteration: 195434\n",
      "Epoch 279 --- Training Accuracy:  79.7%, Validation Accuracy:  78.4%, Train Loss: 0.942 Validation Loss: 0.971\n",
      "Iteration: 196137\n",
      "Epoch 280 --- Training Accuracy:  82.8%, Validation Accuracy:  79.9%, Train Loss: 0.916 Validation Loss: 0.920\n",
      "Iteration: 196840\n",
      "Epoch 281 --- Training Accuracy:  82.8%, Validation Accuracy:  79.7%, Train Loss: 0.842 Validation Loss: 0.935\n",
      "Iteration: 197543\n",
      "Epoch 282 --- Training Accuracy:  84.4%, Validation Accuracy:  78.9%, Train Loss: 0.760 Validation Loss: 0.957\n",
      "Iteration: 198246\n",
      "Epoch 283 --- Training Accuracy:  89.1%, Validation Accuracy:  80.1%, Train Loss: 0.760 Validation Loss: 0.921\n",
      "Iteration: 198949\n",
      "Epoch 284 --- Training Accuracy:  81.2%, Validation Accuracy:  80.0%, Train Loss: 0.886 Validation Loss: 0.926\n",
      "Iteration: 199652\n",
      "Epoch 285 --- Training Accuracy:  79.7%, Validation Accuracy:  79.7%, Train Loss: 0.944 Validation Loss: 0.922\n"
     ]
    }
   ],
   "source": [
    "# Run the optimizer    \n",
    "optimize(num_iterations=200000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading files for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'testing batch 1 of 1'\n"
     ]
    }
   ],
   "source": [
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        data_dict = pickle.load(fo, encoding='bytes')\n",
    "    return data_dict\n",
    "\n",
    "path = 'cifar-10-batches-py'\n",
    "file = []\n",
    "\n",
    "file.append('test_batch')\n",
    "\n",
    "X_test_orig = None\n",
    "Y_test_orig = None\n",
    "\n",
    "for i in range(1):\n",
    "    fname = path+'/'+file[i]\n",
    "    data_dict = unpickle(fname)\n",
    "    \n",
    "    _X = np.array(data_dict[b'data'], dtype=float) / 255.0\n",
    "    _X = _X.reshape([-1, 3, 32, 32])\n",
    "    _X = _X.transpose([0, 2, 3, 1])\n",
    "    _X = _X.reshape(-1, 32*32*3)\n",
    "    _Y = data_dict[b'labels']\n",
    "\n",
    "    X_test_format = _X\n",
    "    Y_test_format = np.array(_Y)\n",
    "    print(data_dict[b'batch_label'])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting test accuracy and saving the npy results file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to restore last checkpoint ...\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/cifar10_cnn-181374\n",
      "Restored checkpoint from: checkpoints/cifar10_cnn-181374\n",
      "Accuracy on test set is:  0.815\n"
     ]
    }
   ],
   "source": [
    "# Restore the session that we saved above for maximum validation accuracy\n",
    "\n",
    "session1 = tf.Session()\n",
    "try:\n",
    "    print(\"Trying to restore last checkpoint ...\")\n",
    "\n",
    "    # Use TensorFlow to find the latest checkpoint - if any.\n",
    "    last_chk_path = tf.train.latest_checkpoint(checkpoint_dir=save_dir)\n",
    "\n",
    "    # Try and load the data in the checkpoint.\n",
    "    saver.restore(session1, save_path=last_chk_path)\n",
    "\n",
    "    # If we get to this point, the checkpoint was successfully loaded.\n",
    "    print(\"Restored checkpoint from:\", last_chk_path)\n",
    "except:\n",
    "    print(\"Failed to restore checkpoint. Initializing variables instead.\")\n",
    "    \n",
    "Y_test_hot = one_hot(Y_test_format)\n",
    "Y_test_hot = Y_test_hot.T\n",
    "feed_dict_test= {x: X_test_format,y_true: Y_test_hot, phase: 0, keep_prob:1.0}\n",
    "acc,pred_y = session1.run([accuracy,y_pred], feed_dict=feed_dict_test)\n",
    "print(\"Accuracy on test set is: \",acc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feed_dict_test= {x: X_test,y_true: Y_test_hot, phase: 0, keep_prob:1.0}\n",
    "acc,pred_y = session1.run([accuracy,y_pred], feed_dict=feed_dict_test)\n",
    "save_predictions(\"ans2-ck2840.npy\",pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
